# Projeto de Engenharia de Dados: Migra√ß√£o, Processamento e Visualiza√ß√£o de um Banco de Dados SQL Server para um Ambiente AWS com Datalake e Power BI

Este projeto envolve a transfer√™ncia de dados de um banco de dados SQL Server para a AWS, sua transforma√ß√£o e posterior visualiza√ß√£o no Power BI. Ao longo deste processo, s√£o utilizadas v√°rias tecnologias e servi√ßos da AWS, como RDS, DMS, Glue e Athena, para garantir uma manipula√ß√£o de dados eficiente e eficaz.




## üíª Sum√°rio
* [Overview da solu√ß√£o](#overview-da-solu√ß√£o)
* [Desafios e potenciais impactos ](#desafios-e-potenciais-impactos)
* [Backup do SQL Server](#backup-do-sql-server)
    + Instala√ß√£o Docker 
    + Cria√ß√£o de um container sql server.
    + Importa√ß√£o do backup para o container
* [Cria√ß√£o de uma inst√¢ncia do RDS Postgres](#cria√ß√£o-de-uma-inst√¢ncia-do-rds-postgres)
* [Migra√ß√£o dos dados do banco Sql Server no Docker para RDS Postgres](#migra√ß√£o-dos-dados-do-banco-sql-server-no-docker-para-rds-postgres)
* [Migra√ß√£o dos dados do RDS para S3 com DMS](#migra√ß√£o-dos-dados-do-RDS-para-S3-com-DMS)
* [Transforma√ß√£o dos dados com AWS Glue](#transforma√ß√£o-dos-dados-com-aws-glue)
* [Armazenamento dos dados processados no S3](#armazenamento-dos-dados-processados-no-s3)
* [Consulta dos dados com o AWS Athena](#consulta-dos-dados-com-o-AWS-Athena)
* [Visualiza√ß√£o dos dados com Power BI](#visualiza√ß√£o-dos-dados-com-Power-BI)
<br>

## Overview da solu√ß√£o
![AWS Glue Job (1)](https://github.com/GustavoGuarany/projeto-ed-tv-jornalismo/assets/126171692/12b86413-927b-4791-9376-6db847c8a65e)
<br>

## Desafios e potenciais impactos 
1. Garantia da integridade e consist√™ncia dos dados
2. Otimiza√ß√£o do desempenho dos recursos
3. Seguran√ßa dos dados durante todo o fluxo
4. Identifica√ß√£o dos rep√≥rteres mais ativos
5. Analise dos locais das mat√©rias e as cidades com maior cobertura
6. Distribui√ß√£o de tipos de mat√©rias
7. Identifica√ß√£o de cinegrafistas e produtores mais ativos
8. An√°lise temporal: identifica√ß√£o de padr√µes sazonais, quantidade e tipos de mat√©rias por per√≠odo 
9. Identifica√ß√£o da subutiliza√ß√£o do sistema
<br>

 
## Backup do SQL Server 
![MicrosoftSQLServer](https://img.shields.io/badge/Microsoft%20SQL%20Server-CC2927?style=for-the-badge&logo=microsoft%20sql%20server&logoColor=white)	![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)

![sql-server-jupyter-postgres](https://github.com/GustavoGuarany/projeto-engenharia-dados-tv-jornalismo/assets/126171692/6c58c6b7-4e4d-4cd4-80a8-248900800aac)

1. Docker
[Instala√ß√£o do docker em ambiente windows](https://docs.docker.com/desktop/install/windows-install/)

2. Cria√ß√£o de um container docker para instalar o sql server e restaurar o backup do banco de dados.

Pull da imagem de cont√™iner do SQL Server mais recente.
```console
docker pull mcr.microsoft.com/mssql/server:2022-latest
```
3. Cria√ß√£o do cont√™iner docker SQL Server usando a imagem
```console
docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=sua_senha_aqui' \
   -p 1433:1433 --name meu-sql-server \
   -v sql1data:/var/opt/mssql\
   -d mcr.microsoft.com/mssql/server:2022-latest
```
4. Movendo o arquivo de backup para o cont√™iner
```console
docker cp /path/to/database.bak  sql_server_conteiner_id:/var/opt/mssql/data/database.bak
```
5. Interagindo com o conteiner em execu√ß√£o.
```console
docker exec -it conteiner_id /bin/bash
```
6. Restaurando o banco de dados utilizando a ferramenta de linha de comando 'sqlcmd'.
```console
/opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P "sua_senha"

RESTORE DATABASE database FROM DISK = 'database.bak'
GO
```
<br>

## Cria√ß√£o de uma inst√¢ncia do RDS Postgres 
![Postgres](https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white)![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=for-the-badge&logo=terraform&logoColor=white)

Utiliza√ß√£o do terraform para provisionamento de recursos e gerenciamento da infraestrutura.
* [Tutorial instal√ß√£o do Terraform no Windows](https://github.com/GustavoGuarany/terraform/blob/main/README.md)

> ‚û°Ô∏è **[C√≥digos Terraform](https://github.com/GustavoGuarany/projeto-engenharia-dados-tv-jornalismo/tree/master/Terraform)**
<br>
  
## Migra√ß√£o dos dados do banco Sql Server no Docker para RDS Postgres 
[![Made withJupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange?style=for-the-badge&logo=Jupyter)](https://jupyter.org/try)![Anaconda](https://img.shields.io/badge/Anaconda-%2344A833.svg?style=for-the-badge&logo=anaconda&logoColor=white)

Migra√ß√£o de dados de um banco SQL Server rodando no cont√™iner Docker para um banco PostgreSQL na AWS RDS.<br><br>
`Python`<br>
`Bibliotecas pyodbc, pandas e sqlalchemy.`<br><br>

**C√≥digo Migra√ß√£o SQL Server para o RDS** ‚¨áÔ∏è
<details>

<summary>migration-sql-server-rds-postgres.py</summary>

```python
import pyodbc
import pandas as pd
import pymysql
from sqlalchemy import create_engine

#Estabelecendo conex√£o com o banco de dados no docker
server = 'server' 
database = 'db' 
username = 'username' 
password = '*****'  
cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)

#Extraindo somente as tabelas que ser√£o utilizadas
query_materia = "SELECT * FROM materia;"
df_materia = pd.read_sql(query_materia, cnxn)

query_usuario = "SELECT * FROM usuario;"
df_usuario = pd.read_sql(query_usuario, cnxn)

query_muni = "SELECT * FROM municipio;"
df_muni = pd.read_sql(query_muni, cnxn)

query_tm = "SELECT * FROM tipomateria;"
df_tm = pd.read_sql(query_tm, cnxn)

query_edi = "SELECT * FROM editoria;"
df_edi = pd.read_sql(query_edi, cnxn)

#Fazendo a ingest√£o no banco postgre do RDS da AWS
engine = create_engine('postgresql://postgres:*****@db.url:porta/banco')
#Dataframes que ser√£o inseridos
dataframes = [df_materia, df_usuario, df_muni, df_tm, df_edi]
#Nomeando as tabelas
nomes_tabelas = ['dbo_materia', 'dbo_usuario','dbo_municipio','dbo_tipomateria','dbo_editoria']
#Inteirando nos dataframes e inserindo os dados no rds
for df, tabela in zip(dataframes, nomes_tabelas):
    df.to_sql(tabela, engine, if_exists='replace')

```

</details>
<br>

 
## Migra√ß√£o dos dados do RDS para S3 com DMS 
![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=for-the-badge&logo=terraform&logoColor=white) 

Cada etapa ser√° realizada via c√≥digos Terraform

Para realizar a migra√ß√£o dos dados da inst√¢ncia RDS para o S3 usando o AWS DMS (Data Migration Service), seguiremos algumas etapas.
1. Configura√ß√£o do IAM: Cria√ß√£o de uma fun√ß√£o de IAM que o AWS DMS utilize para acessar o bucket do S3
2. Cria√ß√£o da inst√¢ncia de replica√ß√£o do DMS: Ser√° respons√°vel por gerenciar a migra√ß√£o do dados
3. Cria√ß√£o do endpoint de origem no DMS: Ser√° a conex√£o com a inst√¢ncia do RDS
4. Cria√ß√£o do endpoint de destino no DMS: Ser√° a conex√£o com o bucket S3
5. Cria√ß√£o de uma tarefa de migra√ß√£o no DMS: A tarefa usar√° a inst√¢ncia de replica√ß√£o, o endpoint de origem e o endpoint de destino para migrar os dados

Ap√≥s a cria√ß√£o e inicia√ß√£o da tarefa de migra√ß√£o, o AWS DMS come√ßar√° a migrar os dados do seu banco de dados RDS para o bucket S3


> ‚û°Ô∏è **[C√≥digos Terraform](https://github.com/GustavoGuarany/projeto-engenharia-dados-tv-jornalismo/tree/master/Terraform)**
<br>
 
## Transforma√ß√£o dos dados com AWS Glue  
‚öôÔ∏è  Prepara√ß√£o e o carregamento dos dados para an√°lise.

**Etapas**
* job ETL: execu√ß√£o do script python    
    + Extra√ß√£o: obten√ß√£o do dados brutos
    + Transforma√ß√£o, limpeza dos dados, remo√ß√£o de duplicatas, preenchimento de valores ausentes, modelagem, normaliza√ß√£o, agrega√ß√£o de dados.
    + Limpeza e valida√ß√£o, verifica√ß√£o de consist√™ncia, integridade e validade
    + Carregamento dos dados no destino final
* Crawler
    + Defini√ß√£o de um crawler para rastrear os dados, criar um banco dentro do catalogo de dados.
    + Crawler name : crawler-tvnews
    + Crawler soucer type: Data stores
    + Repeat crawls od s3 data stores: crawl all folders
    + Escolha um datastore: S3
    + conex√£o: sem conex√£o
    + Rastrear dados no > Caminho especificado na minha conta: s3a://tvnews-curated-prod/tvnews-full
    + Fun√ß√£o do IAM: Crie uma fun√ß√£o do IAM: tvnews-role-crawler
    + Frequ√™ncia: Executar sob demanda
    + Banco de dados: database
    + Prefixo adicionados a tabelas: crawler_tvnews_
    + Op√ß√µes de configura√ß√£o: Atualizar a defini√ß√£o da tabela no cat√°logo de dados
    + Como o AWS Glue deve lidar com a exclus√£o de objetos no datastore?: Marcar a tabela como suspensa no cat√°logo de dados
    + Execu√ß√£o do crawler criado
    + Dados disponiveis no Athena
<br>

**C√≥digo Glue job** ‚¨áÔ∏è
<details>

<summary>glue-job-tvnews.py</summary>

```python
from pyspark.sql.types import *
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

-- setup da aplica√ß√£o Spark
spark = SparkSession \
    .builder \
    .appName("job-glue-spark-tvnews") \
    .getOrCreate()

-- definindo o m√©todo de logging da aplica√ß√£o use INFO somente para DEV [INFO,ERROR]
spark.sparkContext.setLogLevel("ERROR")

df_mat = spark.read.format("csv")\
    .option("header", "True")\
    .option("inferSchema","True")\
    .csv("s3a://tvnews-landing-prod/materia.csv")
df_usu = spark.read.format("csv")\
    .option("header", "True")\
    .option("inferSchema","True")\
    .csv("s3a://tvnews-landing-prod/usuarios.csv")
df_edi = spark.read.format("csv")\
    .option("header", "True")\
    .option("inferSchema","True")\
    .csv("s3a://tvnews-landing-prod/editoria.csv")
df_muni = spark.read.format("csv")\
    .option("header", "True")\
    .option("inferSchema","True")\
    .csv("s3a://tvnews-landing-prod/municipio.csv")
df_tpmat = spark.read.format("csv")\
    .option("header", "True")\
    .option("inferSchema","True")\
    .csv("s3a://tvnews-landing-prod/tipomateria.csv") 


-- converte para formato parquet
print ("\nEscrevendo os dados lidos da raw para parquet na processing zone...")
df_mat.write.format("parquet")\
        .mode("overwrite")\
        .save("s3a://tvnews-processed-prod/dbo_materia.parquet")
df_usu.write.format("parquet")\
        .mode("overwrite")\
        .save("s3a://tvnews-processed-prod/dbo_usuario.parquet")
df_edi.write.format("parquet")\
        .mode("overwrite")\
        .save("s3a://tvnews-processed-prod/dbo_editoria.parquet")
df_muni.write.format("parquet")\
        .mode("overwrite")\
        .save("s3a://tvnews-processed-prod/dbo_municipio.parquet")
df_tpmat.write.format("parquet")\
        .mode("overwrite")\
        .save("s3a://tvnews-processed-prod/dob_tipomateria.parquet")

-- lendo arquivos parquet
df_mat_parquet = spark.read.format("parquet")\
 .load("s3a://tvnews-processed-prod/dbo_materia.parquet/*.parquet")
df_usu_parquet = spark.read.format("parquet")\
 .load("s3a://tvnews-processed-prod/dbo_usuario.parquet/*.parquet")
df_edi_parquet = spark.read.format("parquet")\
 .load("s3a://tvnews-processed-prod/dbo_editoria.parquet/*.parquet")
df_muni_parquet = spark.read.format("parquet")\
 .load("s3a://tvnews-processed-prod/dbo_municipio.parquet/*.parquet")
df_tpmat_parquet = spark.read.format("parquet")\
 .load("s3a://tvnews-processed-prod/dob_tipomateria.parquet/*.parquet")

-- Selecionando apenas as colunas necessarias dos dataframes
df_usu_parquet = df_usu_parquet.select('CODUSUARIO', 'US_NOME')
df_muni_parquet = df_muni_parquet.select('CODMUNICIPIO', 'MU_NOME')
df_edi_parquet = df_edi_parquet.select('CODEDITORIA', 'ED_DESCRICAO')
df_tpmat_parquet = df_tpmat_parquet.select('CODTIPOMATERIA', 'TM_DESCRICAO')

-- Substituindo o codigo dos reportes, produtores, cinegrafistas pelo nome
df_joined = df_mat_parquet.join(df_usu_parquet, df_mat_parquet.CODREPORTER == df_usu_parquet.CODUSUARIO, 'left').withColumnRenamed('US_NOME', 'REPORTER').withColumnRenamed('CODUSUARIO', 'CODREPORTER_')
df_joined = df_joined.join(df_usu_parquet, df_mat_parquet.CODPRODUTOR == df_usu_parquet.CODUSUARIO, 'left').withColumnRenamed('US_NOME', 'PRODUTOR').withColumnRenamed('CODUSUARIO', 'CODPRODUTOR_')
df_joined = df_joined.join(df_usu_parquet, df_mat_parquet.CODCINEGRAFISTA == df_usu_parquet.CODUSUARIO, 'left').withColumnRenamed('US_NOME', 'CINEGRAFISTA').withColumnRenamed('CODUSUARIO', 'CODPRODUTOR_')

-- Substituindo o codigo dos municipios, editoria, tipo da materia por seus respectivos nomes
df_joined = df_joined.join(df_muni_parquet, df_mat_parquet.CODMUNICIPIO == df_muni_parquet.CODMUNICIPIO, 'left').withColumnRenamed('MU_NOME', 'MUNICIPIO').withColumnRenamed('CODMUNICIPIO', 'CODREPORTER_')\
                     .join(df_edi_parquet, df_mat_parquet.CODEDITORIA == df_edi_parquet.CODEDITORIA, 'left').withColumnRenamed('ED_DESCRICAO', 'EDITORIA').withColumnRenamed('CODEDITORIA', 'CODEDITORIA_')\
                     .join(df_tpmat_parquet, df_mat_parquet.CODTIPOMATERIA == df_tpmat_parquet.CODTIPOMATERIA, 'left').withColumnRenamed('TM_DESCRICAO', 'TIPO_MATERIA').withColumnRenamed('CODTIPOMATERIA', 'CODTIPOMATERIA_')

df_soft = df_joined.select('CODMATERIA', 'PRODUTOR','REPORTER','CINEGRAFISTA','MUNICIPIO','TIPO_MATERIA','EDITORIA','MA_DATA','MA_LOCAL','MA_RETRANCA')

df_soft = df_soft.withColumn("MA_LOCAL", when(col("MA_LOCAL") == 'POL√çCIA', 'DELEGACIA').otherwise(col("MA_LOCAL")))\
                   .withColumn("MA_LOCAL", when(col("MA_LOCAL") == 'POLICIA', 'DELEGACIA').otherwise(col("MA_LOCAL")))\
                   .withColumn("MA_LOCAL", when(col("MA_LOCAL") == 'EST√ÅDIO BARBALH√ÉO', 'EST√ÅDIO ').otherwise(col("MA_LOCAL")))\
                   .withColumn("MA_LOCAL", when(col("MA_LOCAL") == 'COLOSSO DO TAPAJ√ìS', 'EST√ÅDIO ').otherwise(col("MA_LOCAL")))\
                   .withColumn("MA_LOCAL", when(col("MA_LOCAL") == 'C√ÇMARA', 'C√ÇMARA DE VEREADORES').otherwise(col("MA_LOCAL")))

-- Limpeza na coluna MA_LOCAL 
df_final = df_soft.withColumn('MA_LOCAL', when(col('MA_LOCAL').rlike('X{2,}'), 'NAO ESPECIFICADO').otherwise(col('MA_LOCAL')))\
                               .withColumn('MA_LOCAL', when(col('MA_LOCAL').rlike('\.{2,}'), 'NAO ESPECIFICADO').otherwise(col('MA_LOCAL')))\
                               .withColumn('MA_LOCAL', when(col('MA_LOCAL').rlike('\,{2,}'), 'NAO ESPECIFICADO').otherwise(col('MA_LOCAL')))\
                               .withColumn('MA_LOCAL', when(col('MA_LOCAL').rlike('\\*,{2,}'), 'NAO ESPECIFICADO').otherwise(col('MA_LOCAL')))\
                               .withColumn('MA_LOCAL', when(col('MA_LOCAL').rlike('Z{2,}'), 'NAO ESPECIFICADO').otherwise(col('MA_LOCAL')))\
                               .withColumn('MA_LOCAL', when(col('MA_LOCAL').rlike('\;{2,}'), 'NAO ESPECIFICADO').otherwise(col('MA_LOCAL')))\
                               .withColumn('MA_LOCAL', when(col('MA_LOCAL').rlike('-{2,}'), 'NAO ESPECIFICADO').otherwise(col('MA_LOCAL')))


-- Crie uma lista das palavras que voc√™ deseja substituir
palavras_para_substituir = [ 'NA PAURTA',r'\bS\b',r'\bN PAUTA\b',r'\b-\b','NA PAUTA', r'\b//\b','NA PAUTA//','SANTAR√âM','SANTAREM','TV','STM','V√ÅRIOS','///','/','VER PAUTA','VER PAUTA','INDEFINIDO',r'\bRUA\b','VER NA PAUTA','RUAS','A DEFINIR','\\.', r'\b;\b', r'\bV√çDEO\b','\\*+',r'\bR\b',r'\bRR\b','RRR','SSSS',r'\bD\b','AAA',r'\bAA\b','////','SSS',r'\bSS\b',r'\bQ\b',r'\bVARIOS\b','LLLL','NA ¬¥PAUTA','N APAUTA','NA OPAUTA','GGG','NA PAUTAS']  # note que estamos escapando o ponto

-- Crie a express√£o regular
regex = '|'.join(palavras_para_substituir)

-- Substitua a coluna se ela contiver qualquer uma das palavras na lista
df_final = df_final.withColumn('MA_LOCAL', when(col('MA_LOCAL').rlike(regex), 'NAO ESPECIFICADO').otherwise(col('MA_LOCAL')))

df_final.show(truncate=False)

df_final.repartition(1)\
          .write\
          .format("parquet")\
          .mode("overwrite")\
          .save("s3a://tvnews-curated-prod/tvnews-full.parquet/*.parquet")
```
</details>
<br>
   
O processo de ETL pode ser cont√≠nuo, permitindo que os dados sejam atualizados e refinados ao longo do tempo.

<br><br>

![crawler](https://github.com/GustavoGuarany/projeto-ed-tv-jornalismo/assets/126171692/76835da7-01bb-45fb-9763-9b7fb20ca9ec)

<br><br>

## Armazenamento dos dados processados no S3 

üåê Os dados ser√£o organizados e armazenados em tr√™s buckets distintos: Landing, Processing e Curated, todos integrantes de um sistema de Data Lake.

> ü™£ **Bucket Landing**: O armazenamento dos dados em seu estado bruto, em formato csv, sem nenhum tipo de manipula√ß√£o ou processamento.

> ü™£ **Bucket Processing**: Armazena os dados convertidos para o formato Parquet.

> ü™£ **Bucket Curated**: Armazena o resultado final do processo de ETL, nessa etapa, os dados j√° foram limpos, transformados, validados, modelados e est√£o prontos para serem consumidos e usados para gerar valor para a organiza√ß√£o.

Em conjunto, esses tr√™s buckets formam a estrutura do Data Lake, permitindo um fluxo de dados eficiente e bem organizado, que facilita a manipula√ß√£o, an√°lise e utiliza√ß√£o desses dados

**Buckets Provisionados via Terraform**
> ‚û°Ô∏è **[C√≥digos Terraform](https://github.com/GustavoGuarany/projeto-engenharia-dados-tv-jornalismo/tree/master/Terraform)**
<br>

## Consulta dos dados com o AWS Athena 

‚ö° Servi√ßo interativo de consultas que facilita a an√°lise de dados diretamente no Amazon S3 usando SQL padr√£o

Ap√≥s a execu√ß√£o do crawler o aws glue cria uma tabela no AwsDataCatalog e essa tabela fica dispon√≠vel no editor do Athena.

Configura√ß√µes > Manage Settings > Direcione o resultado das consultas para o backet athena-query-tvnews

<br><br>

**C√≥digo para consultar as tabelas diretamente no Athena** ‚¨áÔ∏è
<details>
<summary>querys-athena-tvnews.sql</summary>
    
```sql
-- Consultando as editorias com mais ocorrencias em 2022
SELECT extract(year from MA_DATA) AS ANO, EDITORIA, COUNT(*) AS TOTAL
FROM crawler_bucket_tvnews_prod
WHERE extract(year from MA_DATA) = 2022
GROUP BY extract(year from MA_DATA), EDITORIA
ORDER BY TOTAL DESC

-- Reporter que mais colaborar√£o no ano de 2023
WITH data AS (
  SELECT extract(year from MA_DATA) AS ANO, REPORTER
  FROM crawler_bucket_tvnews_prod
)
SELECT ANO, REPORTER, COUNT(*) AS TOTAL
FROM data
WHERE ANO = 2023
GROUP BY ANO, REPORTER
ORDER BY TOTAL DESC

-- Reporter e cinegrafista que mais trabalharam juntos no ano de 2023
WITH data AS (
  SELECT extract(year from MA_DATA) AS ANO, REPORTER, CINEGRAFISTA
  FROM dados
)
SELECT REPORTER, CINEGRAFISTA, COUNT(*) AS TOTAL
FROM data
WHERE ANO = 2023
GROUP BY REPORTER, CINEGRAFISTA
ORDER BY TOTAL DESC
LIMIT 10

-- Rep√≥rter e editoria e ordenados em ordem decrescente pelo n√∫mero total de mat√©rias
WITH data AS (
  SELECT extract(year from MA_DATA) AS ANO, REPORTER, EDITORIA
  FROM dados
)
SELECT REPORTER, EDITORIA, COUNT(*) AS TOTAL_MAT
FROM data
WHERE ANO = 2023
GROUP BY REPORTER, EDITORIA
ORDER BY TOTAL_MAT DESC
```

</details>

<br>

![tela-athena_borra](https://github.com/GustavoGuarany/projeto-ed-tv-jornalismo/assets/126171692/f3f7302d-be59-443a-9a75-1fa0333c8d5c)

<br>
 
## Visualiza√ß√£o dos dados com Power BI

![Power Bi](https://img.shields.io/badge/power_bi-F2C811?style=for-the-badge&logo=powerbi&logoColor=black)

 üìä üìà Setup Athena + Power BI
 <br>
* Conex√£o ao Amazon Athena com ODBC
    + Baixar Simba Athena ODBC Driver
        + Fonte de dados ODBC >> DSN de Sistema >> Simba Athena ODBC drive >> Configurar o Simba com os dados corretos >> configurar as op√ß√µes de autentica√ß√£o >> No Power BI >> Obter dados >> ODBC >> DNS 
<br>

**Visualiza√ß√µes**

![tela-01-powerbi_borra](https://github.com/GustavoGuarany/projeto-ed-tv-jornalismo/assets/126171692/b4687189-01be-4219-86a5-d61659bd4402)

<br><br>

![tela-02-powerbi_borra](https://github.com/GustavoGuarany/projeto-ed-tv-jornalismo/assets/126171692/8efad900-5b42-4085-907a-0692b5afafb5)

<br><br>

![tela-03-powerb_borra](https://github.com/GustavoGuarany/projeto-ed-tv-jornalismo/assets/126171692/6a90a8be-d3de-4412-b2f8-1aab66dedf80)

<br><br>








