# Projeto de Engenharia de Dados: Migra√ß√£o, Processamento e Visualiza√ß√£o de um Banco de Dados SQL Server para um Ambiente AWS com Datalake e Power BI

Este projeto envolve a transfer√™ncia de dados de um banco de dados SQL Server para a AWS, sua transforma√ß√£o e posterior visualiza√ß√£o no Power BI. Ao longo deste processo, s√£o utilizadas v√°rias tecnologias e servi√ßos da AWS, como RDS, DMS, Glue e Athena, para garantir uma manipula√ß√£o de dados eficiente e eficaz.




## üíª Sum√°rio
* [Overview da solu√ß√£o](#overview-da-solu√ß√£o)
* [Desafios e potenciais impactos ](#desafios-e-potenciais-impactos)
* [Backup do SQL Server](#backup-do-sql-server)
    + Instala√ß√£o Docker 
    + Cria√ß√£o de um container sql server.
    + Importa√ß√£o do backup para o container
* [Cria√ß√£o de uma inst√¢ncia do RDS Postgres](#cria√ß√£o-de-uma-inst√¢ncia-do-rds-postgres)
* [Migra√ß√£o dos dados do banco Sql Server no Docker para RDS Postgres](#migra√ß√£o-dos-dados-do-banco-sql-server-no-docker-para-rds-postgres)
* [Migra√ß√£o dos dados do RDS para S3 com DMS](#migra√ß√£o-dos-dados-do-RDS-para-S3-com-DMS)
* [Transforma√ß√£o dos dados com AWS Glue](#transforma√ß√£o-dos-dados-com-aws-glue)
* [Armazenamento dos dados processados no S3](#armazenamento-dos-dados-processados-no-s3)
* [Consulta dos dados com o AWS Athena](#consulta-dos-dados-com-o-AWS-Athena)
* [Visualiza√ß√£o dos dados com Power BI](#visualiza√ß√£o-dos-dados-com-Power-BI)


## Overview da solu√ß√£o
![AWS Glue Job (1)](https://github.com/GustavoGuarany/projeto-ed-tv-jornalismo/assets/126171692/12b86413-927b-4791-9376-6db847c8a65e)


## Desafios e potenciais impactos 
1. Garantia da integridade e consist√™ncia dos dados
2. Otimiza√ß√£o do desempenho dos recursos
3. Seguran√ßa dos dados durante todo o fluxo
4. Identifica√ß√£o dos rep√≥rteres mais ativos
5. Analise dos locais das mat√©rias e as cidades com maior cobertura
6. Distribui√ß√£o de tipos de mat√©rias
7. Identifica√ß√£o de cinegrafistas e produtores mais ativos
8. An√°lise temporal: identifica√ß√£o de padr√µes sazonais, quantidade e tipos de mat√©rias por per√≠odo 
9. Identifica√ß√£o da subutiliza√ß√£o do sistema


 
## Backup do SQL Server 
![MicrosoftSQLServer](https://img.shields.io/badge/Microsoft%20SQL%20Server-CC2927?style=for-the-badge&logo=microsoft%20sql%20server&logoColor=white)	![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)

1. Docker
[Instala√ß√£o do docker em ambiente windows](https://docs.docker.com/desktop/install/windows-install/)

2. Cria√ß√£o de um container docker para instalar o sql server e restaurar o backup do banco de dados.

Pull da imagem de cont√™iner do SQL Server mais recente.
```console
docker pull mcr.microsoft.com/mssql/server:2022-latest
```
3. Cria√ß√£o do cont√™iner docker SQL Server usando a imagem
```console
docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=sua_senha_aqui' \
   -p 1433:1433 --name meu-sql-server \
   -v sql1data:/var/opt/mssql\
   -d mcr.microsoft.com/mssql/server:2022-latest
```
4. Movendo o arquivo de backup para o cont√™iner
```console
docker cp /path/to/database.bak  sql_server_conteiner_id:/var/opt/mssql/data/database.bak
```
5. Interagindo com o conteiner em execu√ß√£o.
```console
docker exec -it conteiner_id /bin/bash
```
6. Restaurando o banco de dados utilizando a ferramenta de linha de comando 'sqlcmd'.
```console
/opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P "sua_senha"

RESTORE DATABASE database FROM DISK = 'database.bak'
GO
```


## Cria√ß√£o de uma inst√¢ncia do RDS Postgres 
![Postgres](https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white)![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=for-the-badge&logo=terraform&logoColor=white)

Utiliza√ß√£o do terraform para provisionamento de recursos e gerenciamento da infraestrutura.
* [Tutorial instal√ß√£o do Terraform no Windows](https://github.com/GustavoGuarany/terraform/blob/main/README.md)
* C√≥digo para provisionamento do RDS Postgres via Terraform link:

  
## Migra√ß√£o dos dados do banco Sql Server no Docker para RDS Postgres 
[![Made withJupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange?style=for-the-badge&logo=Jupyter)](https://jupyter.org/try)![Anaconda](https://img.shields.io/badge/Anaconda-%2344A833.svg?style=for-the-badge&logo=anaconda&logoColor=white)

Migra√ß√£o de dados de um banco SQL Server rodando no cont√™iner Docker para um banco PostgreSQL na AWS RDS.<br><br>
`Python`<br>
`Bibliotecas pyodbc, pandas e sqlalchemy.`<br><br>
c√≥digo python link:

 
## Migra√ß√£o dos dados do RDS para S3 com DMS 
![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=for-the-badge&logo=terraform&logoColor=white) 

Cada etapa ser√° realizada via c√≥digos Terraform

Para realizar a migra√ß√£o dos dados da inst√¢ncia RDS para o S3 usando o AWS DMS (Data Migration Service), seguiremos algumas etapas.
1. Configura√ß√£o do IAM: Cria√ß√£o de uma fun√ß√£o de IAM que o AWS DMS utilize para acessar o bucket do S3
2. Cria√ß√£o da inst√¢ncia de replica√ß√£o do DMS: Ser√° respons√°vel por gerenciar a migra√ß√£o do dados
3. Cria√ß√£o do endpoint de origem no DMS: Ser√° a conex√£o com a inst√¢ncia do RDS
4. Cria√ß√£o do endpoint de destino no DMS: Ser√° a conex√£o com o bucket S3
5. Cria√ß√£o de uma tarefa de migra√ß√£o no DMS: A tarefa usar√° a inst√¢ncia de replica√ß√£o, o endpoint de origem e o endpoint de destino para migrar os dados

Ap√≥s a cria√ß√£o e inicia√ß√£o da tarefa de migra√ß√£o, o AWS DMS come√ßar√° a migrar os dados do seu banco de dados RDS para o bucket S3


C√≥digos terraform link:

 
## Transforma√ß√£o dos dados com AWS Glue  
‚öôÔ∏è  Prepara√ß√£o e o carregamento dos dados para an√°lise.

**Etapas**
* job ETL: execu√ß√£o do script python    
    + Extra√ß√£o: obten√ß√£o do dados brutos
    + Transforma√ß√£o, limpeza dos dados, remo√ß√£o de duplicatas, preenchimento de valores ausentes, modelagem, normaliza√ß√£o, agrega√ß√£o de dados.
    + Limpeza e valida√ß√£o, verifica√ß√£o de consist√™ncia, integridade e validade
    + Carregamento dos dados no destino final
* Crawler
    + Defini√ß√£o de um crawler para rastrear os dados, criar um banco dentro do catalogo de dados.
    + Crawler name : crawler-tvnews
    + Crawler soucer type: Data stores
    + Repeat crawls od s3 data stores: crawl all folders
    + Escolha um datastore: S3
    + conex√£o: sem conex√£o
    + Rastrear dados no > Caminho especificado na minha conta: s3a://tvnews-curated-prod/tvnews-full
    + Fun√ß√£o do IAM: Crie uma fun√ß√£o do IAM: tvnews-role-crawler
    + Frequ√™ncia: Executar sob demanda
    + Banco de dados: database
    + Prefixo adicionados a tabelas: crawler_tvnews_
    + Op√ß√µes de configura√ß√£o: Atualizar a defini√ß√£o da tabela no cat√°logo de dados
    + Como o AWS Glue deve lidar com a exclus√£o de objetos no datastore?: Marcar a tabela como suspensa no cat√°logo de dados
    + Execu√ß√£o do crawler criado
    + Dados disponiveis no Athena


c√≥digo glue-job-tvnews
   
O processo de ETL pode ser cont√≠nuo, permitindo que os dados sejam atualizados e refinados ao longo do tempo.


## Armazenamento dos dados processados no S3 

üåê Os dados ser√£o organizados e armazenados em tr√™s buckets distintos: Landing, Processing e Curated, todos integrantes de um sistema de Data Lake.

> ü™£ **Bucket Landing**: O armazenamento dos dados em seu estado bruto, em formato csv, sem nenhum tipo de manipula√ß√£o ou processamento.

> ü™£ **Bucket Processing**: Armazena os dados convertidos para o formato Parquet.

> ü™£ **Bucket Curated**: Armazena o resultado final do processo de ETL, nessa etapa, os dados j√° foram limpos, transformados, validados, modelados e est√£o prontos para serem consumidos e usados para gerar valor para a organiza√ß√£o.

Em conjunto, esses tr√™s buckets formam a estrutura do Data Lake, permitindo um fluxo de dados eficiente e bem organizado, que facilita a manipula√ß√£o, an√°lise e utiliza√ß√£o desses dados


## Consulta dos dados com o AWS Athena 

‚ö° Servi√ßo interativo de consultas que facilita a an√°lise de dados diretamente no Amazon S3 usando SQL padr√£o

Ap√≥s a execu√ß√£o do crawler o aws glue cria uma tabela no AwsDataCatalog e essa tabela fica dispon√≠vel no editor do Athena.

Configura√ß√µes > Manage Settings > Direcione o resultado das consultas para o backet athena-query-tvnews

* C√≥digo para consultar as tabelas diretamente no Athena.

<details>
<summary>querys-athena-tvnews.sql</summary>
    
```sql
-- Consultando as editorias com mais ocorrencias em 2022
SELECT extract(year from MA_DATA) AS ANO, EDITORIA, COUNT(*) AS TOTAL
FROM crawler_bucket_tvnews_prod
WHERE extract(year from MA_DATA) = 2022
GROUP BY extract(year from MA_DATA), EDITORIA
ORDER BY TOTAL DESC

-- Reporter que mais colaborar√£o no ano de 2023
WITH data AS (
  SELECT extract(year from MA_DATA) AS ANO, REPORTER
  FROM crawler_bucket_tvnews_prod
)
SELECT ANO, REPORTER, COUNT(*) AS TOTAL
FROM data
WHERE ANO = 2023
GROUP BY ANO, REPORTER
ORDER BY TOTAL DESC

-- Reporter e cinegrafista que mais trabalharam juntos no ano de 2023
WITH data AS (
  SELECT extract(year from MA_DATA) AS ANO, REPORTER, CINEGRAFISTA
  FROM dados
)
SELECT REPORTER, CINEGRAFISTA, COUNT(*) AS TOTAL
FROM data
WHERE ANO = 2023
GROUP BY REPORTER, CINEGRAFISTA
ORDER BY TOTAL DESC
LIMIT 10

-- Rep√≥rter e editoria e ordenados em ordem decrescente pelo n√∫mero total de mat√©rias
WITH data AS (
  SELECT extract(year from MA_DATA) AS ANO, REPORTER, EDITORIA
  FROM dados
)
SELECT REPORTER, EDITORIA, COUNT(*) AS TOTAL_MAT
FROM data
WHERE ANO = 2023
GROUP BY REPORTER, EDITORIA
ORDER BY TOTAL_MAT DESC
```

</details>

 
## Visualiza√ß√£o dos dados com Power BI

 üìä üìà Setup Athena + Power BI
* Conex√£o ao Amazon Athena com ODBC
    + Baixar Simba Athena ODBC Driver
        + Fonte de dados ODBC >> DSN de Sistema >> Simba Athena ODBC drive >> Configurar o Simba com os dados corretos >> configurar as op√ß√µes de autentica√ß√£o >> No Power BI >> Obter dados >> ODBC >> DNS 
               







